{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c5f1806",
   "metadata": {},
   "source": [
    "This post elaborates on the workings of Variational Autoencoders or in its abbreviated form VAE. The concept of VAEs is then shown using the example of image data as well as sound data. Particularly, we are showing how one could use a VAE to create new sound of different genres, or to create new product designs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad4da6",
   "metadata": {},
   "source": [
    "# Workings of a Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c03e63",
   "metadata": {},
   "source": [
    "# Mathematical Underlyings of a VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1785e",
   "metadata": {},
   "source": [
    "# Code Implementation of a VAE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9de53c",
   "metadata": {},
   "source": [
    "# Using a VAE for creating images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0daeb7",
   "metadata": {},
   "source": [
    "# Using a VAE for creating sounds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db182d8",
   "metadata": {},
   "source": [
    "Next to images we are also applying the Variational Autoencoder to sound data. For that we are taking sound snippets from the music genres techno, rock and piano music. As usually done with sound-analysis, we are taking a larger piece of music and chop it down into small chunks. For that we downloaded three four-hour youtube compilations of each genre and preprocessed them using the <code>pydub</code> Python package. The length of each sound-chunk is set to 1000ms. This generation of chunks generates more than 10.000 sound snippets for each genre.\n",
    "\n",
    "Processing audio data is not as straightforward as for example image data, where we simply normalize the data and are pretty much done. Audio on the other hand is much more complicated. The questions even start with how are we translating sound into a numeric format. In the following we are elaborating in a concise manner how sound-data is processed and what the features are we are feeding in the end into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cdbd21",
   "metadata": {},
   "source": [
    "# Github Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1c324b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
